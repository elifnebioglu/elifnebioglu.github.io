<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>This project explores <strong>Empirical Bayes Variational Autoencoders (EBVAEs)</strong> through both theory and practice.<br> We study the excess risk bounds introduced by Tang &amp; Yang (2021), and experimentally evaluate how learning an expressive latent prior and decoder noise parameter affects VAE performance.</p> <p>Some key points:</p> <ul> <li>Reproduced and interpreted the <strong>oracle-style excess risk bounds</strong> for EBVAEs.</li> <li>Implemented four VAE variants to test the effect of <strong>learning the latent covariance</strong> and <strong>decoder noise parameter</strong>.</li> <li>Demonstrated that <strong>learning the noise parameter improves EBVAE loss</strong>, even on MNIST.</li> <li>Analyzed reconstruction loss, KL divergence, and generative quality across the four EBVAE variants.</li> </ul> <hr> <h2 id="empirical-bayes-variational-autoencoders-ebvaes">Empirical Bayes Variational Autoencoders (EBVAEs)</h2> <p>EBVAEs generalize standard VAEs by <strong>learning the prior distribution</strong> π(z) instead of assuming a fixed Gaussian.<br> This empirical Bayes viewpoint allows encoder, decoder, and prior parameters to be optimized jointly:</p> <p>[ (\hat p, \hat q, \hat \pi) = \arg\min_{p,q,\pi} \frac{1}{n}\sum_{i=1}^n m(p,q,\pi,x_i) ]</p> <p>The theoretical work provides a <strong>high-probability excess risk bound</strong> that quantifies the gap between:</p> <ul> <li>the empirical minimizer of the EBVAE loss, and</li> <li>the population-level optimal model.</li> </ul> <div class="row justify-content-sm-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/bayesian_dl/ebvae_theory-480.webp 480w,/assets/img/bayesian_dl/ebvae_theory-800.webp 800w,/assets/img/bayesian_dl/ebvae_theory-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/bayesian_dl/ebvae_theory.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="EBVAE theoretical structure" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The EBVAE framework augments VAEs with a learned prior, enabling tighter theoretical guarantees. </div> <p>Key theoretical insights:</p> <ul> <li>Learning the prior helps reduce <strong>approximation error</strong>.</li> <li>Model complexity (encoder/decoder/prior families) controls <strong>estimation error</strong> via Rademacher complexity.</li> <li>For expressive enough neural families and smooth data distributions, the <strong>approximation error can vanish</strong>.</li> </ul> <hr> <h2 id="applications-to-gaussian-vaes">Applications to Gaussian VAEs</h2> <p>We examine the theory in the case where:</p> <ul> <li>the encoder outputs a full covariance Gaussian,</li> <li>the decoder outputs a quantized Gaussian with learned σ,</li> <li>and the prior belongs to a Gaussian family satisfying regularity conditions.</li> </ul> <p>Under these assumptions, the paper shows:</p> <ul> <li>The EBVAE estimator achieves a <strong>log(n)-rate excess risk bound</strong>.</li> <li>Learning the full covariance matrix can reduce approximation error but may hurt estimation error if overly complex.</li> </ul> <hr> <h2 id="experiments-on-mnist">Experiments on MNIST</h2> <p>We implement <strong>four types of EBVAEs</strong>, each resembling a standard VAE but with different learned components:</p> <ol> <li> <strong>Standard VAE</strong> (diagonal latent covariance, fixed decoder noise)</li> <li><strong>Learned latent covariance</strong></li> <li><strong>Learned decoder noise parameter σ</strong></li> <li><strong>Learned covariance + learned noise</strong></li> </ol> <p>To speed up computation, images are downsampled to <strong>14×14</strong>, and models are trained on <strong>1000 samples</strong>.</p> <div class="row justify-content-sm-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/bayesian_dl/ebvae_reconstruction-480.webp 480w,/assets/img/bayesian_dl/ebvae_reconstruction-800.webp 800w,/assets/img/bayesian_dl/ebvae_reconstruction-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/bayesian_dl/ebvae_reconstruction.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="EBVAE Reconstructions" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Reconstructions produced by four EBVAE variants. </div> <p>Findings:</p> <ul> <li> <strong>Learning the noise parameter</strong> improves reconstruction quality and EBVAE loss.</li> <li> <strong>Learning the full covariance</strong> slightly improves KL divergence but worsens EBVAE loss.</li> <li>VAEs with learned covariance tend to produce <strong>overly similar samples</strong> due to latent space irregularities.</li> </ul> <hr> <h2 id="results--discussion">Results &amp; Discussion</h2> <div class="row justify-content-sm-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/bayesian_dl/ebvae_metrics-480.webp 480w,/assets/img/bayesian_dl/ebvae_metrics-800.webp 800w,/assets/img/bayesian_dl/ebvae_metrics-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/bayesian_dl/ebvae_metrics.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Reconstruction &amp; KL metrics" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Reconstruction loss and KL divergence for the four EBVAE configurations. </div> <p>Summary:</p> <ul> <li>Best EBVAE loss: <strong>model with learned noise</strong> </li> <li>Best KL divergence: <strong>model with learned covariance</strong> </li> <li>Best generative quality: <strong>standard VAE</strong> and <strong>noise-learning VAE</strong> </li> </ul> <p>These results support the theoretical insight that <strong>both the covariance structure and decoder noise affect the estimation/approximation trade-off</strong>.</p> <p>Future directions:</p> <ul> <li>Testing on more complex datasets</li> <li>Trying non-Gaussian priors (e.g., mixtures)</li> <li>Investigating regularization strategies to stabilize learned covariance models</li> </ul> <hr> </body></html>